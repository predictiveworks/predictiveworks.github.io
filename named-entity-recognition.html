<!doctype html>
<html>
  <head>
    <meta charset="utf-8">
    <title>Named Entity Recognition</title>
    <link rel="stylesheet" href="/assets/css/fontawesome-free-5.10.1-web/css/all.min.css"/>
    <link rel="stylesheet" href="/assets/css/styles.css">
    <link rel="stylesheet" href="/assets/css/syntax.css">
  </head>
  <body>
    <div class="site-wrapper">

      
<div class="site-header">

  <!-- title -->
  <div class="site-header-title">
    <div class="column-left"></div>
    <div class="column-content">
      <div class="header-content">
        <div class="logo"></div>
        <div class="header-title">
          <span class="brand-grey">Predictive</span><span class="brand-teal">Works.</span>
        </div>
      </div>
    </div>
    <div class="column-right"></div>
  </div>

  <!-- breadcrumbs -->
  <div class="site-header-breadcrumbs">
    <div class="column-left"></div>
    <div class="column-content">
      
        <ul class="breadcrumb">
          <li><a href="/">PredictiveWorks</a> </li>
          
          
            <li class="active">
              
              <i class="fa fa-chevron-right fa-icon"></i>
              named entity recognition
              
            </li>
          
        </ul>
      
    </div>
    <div class="column-right"></div>
  </div>

</div>


      <div class="site-content">
        <div class="column-left"></div>
        <div class="column-content">
          <div class="main-content">
            <div class="main-content-nav">
              <nav class="nav-main">
  <div class="nav-container">
    <!-- concepts -->
    <div class="nav-title">Basic Concepts</div>

      <a href="/what-is-predictiveworks.html">
        <div class="nav-article">What is PredictiveWorks?</div>
      </a>

      <a href="/pluggable-analytics.html">
        <div class="nav-article">Pluggable Analytics</div>
      </a>

      <a href="/maturity-model.html">
        <div class="nav-article">Maturity Model</div>
      </a>

      <a href="/what-others-do.html">
        <div class="nav-article">What Others Do</div>
      </a>

      <a href="/technology-stack.html">
        <div class="nav-article">Technology Stack</div>
      </a>

    <!-- connectors -->
    <div class="nav-title">Data Connectors</div>

      <a href="/built-in-connectors.html">
        <div class="nav-article">Built-in Connectors</div>
      </a>

      <a href="/purpose-built-connectors.html">
        <div class="nav-article">Purpose-built Connectors</div>
      </a>

    <!-- rules & queries -->
    <div class="nav-title">Rules & Queries</div>

      <a href="/drools-rules.html">
        <div class="nav-article">Drools Rules</div>
      </a>

      <a href="/spark-sql.html">
        <div class="nav-article">Spark SQL</div>
      </a>

    <!-- machine learning -->
    <div class="nav-title">Machine Learning</div>

      <a href="/machine-learning.html">
        <div class="nav-article">Machine Learning</div>
      </a>

      <a href="/feature-engineering.html">
        <div class="nav-article">Feature Engineering</div>
      </a>

      <a href="/anomaly-detection.html">
        <div class="nav-article">Anomaly Detection</div>
      </a>

      <a href="/classification.html">
        <div class="nav-article">Classification</div>
      </a>

      <a href="/clustering.html">
        <div class="nav-article">Clustering</div>
      </a>

      <a href="/data-mining.html">
        <div class="nav-article">Data Mining</div>
      </a>

      <a href="/recommendation.html">
        <div class="nav-article">Recommendation</div>
      </a>

      <a href="/regression.html">
        <div class="nav-article">Regression</div>
      </a>

      <a href="/smote-sampling.html">
        <div class="nav-article">SMOTE Sampling</div>
      </a>

    <!-- deep learning -->
    <div class="nav-title">Deep Learning</div>

      <a href="/deep-learning.html">
        <div class="nav-article">Deep Learning</div>
      </a>

    <!-- timeseries -->
    <div class="nav-title">Time Series</div>

      <a href="/time-series-analysis.html">
        <div class="nav-article">Time Series Analysis</div>
      </a>

      <a href="/time-preprocessing.html">
        <div class="nav-article">Preprocessing</div>
      </a>

      <a href="/arima.html">
        <div class="nav-article">ARIMA</div>
      </a>

      <a href="/arma.html">
        <div class="nav-article">ARMA</div>
      </a>

      <a href="/autoregression.html">
        <div class="nav-article">Auto Regression</div>
      </a>

      <a href="/demand-prediction.html">
        <div class="nav-article">Demand Prediction</div>
      </a>

      <a href="/moving-average.html">
        <div class="nav-article">Moving Average</div>
      </a>

      <a href="/stl-decomposition.html">
        <div class="nav-article">STL Decomposition</div>
      </a>

      <a href="/time-regression.html">
        <div class="nav-article">Time Regression</div>
      </a>

    <!-- natural language processing -->
    <div class="nav-title">Natural Language Processing</div>

      <a href="/text-analysis.html">
        <div class="nav-article">Text Analysis</div>
      </a>

      <a href="/text-preprocessing.html">
        <div class="nav-article">Preprocessing</div>
      </a>

      <a href="/text-chunking.html">
        <div class="nav-article">Chunking</div>
      </a>

      <a href="/dependency-parsing.html">
        <div class="nav-article">Dependency Parsing</div>
      </a>

      <a href="/text-embeddings.html">
        <div class="nav-article">Embeddings</div>
      </a>

      <a href="/text-lemmatization.html">
        <div class="nav-article">Lemmatization</div>
      </a>

      <a href="/named-entity-recognition.html">
        <div class="nav-article">Named Entity Recognition</div>
      </a>

      <a href="/part-of-speech-tagging.html">
        <div class="nav-article">Part of Speech Tagging</div>
      </a>

      <a href="/sentiment-analysis.html">
        <div class="nav-article">Sentiment Analysis</div>
      </a>

      <a href="/spell-checking.html">
        <div class="nav-article">Spell Checking</div>
      </a>

      <a href="/text-clustering.html">
        <div class="nav-article">Text Clustering</div>
      </a>

      <a href="/text-matching.html">
        <div class="nav-article">Text Matching</div>
      </a>

      <a href="/topic-modeling.html">
        <div class="nav-article">Topic Modeling</div>
      </a>

    <!-- model management -->
    <div class="nav-title">Model Management</div>

      <a href="/model-management.html">
        <div class="nav-article">Model Management</div>
      </a>

    <!-- model visualization -->
    <div class="nav-title">Model Visualization</div>

      <a href="/grafana-connector.html">
        <div class="nav-article">Grafana Connector</div>
      </a>

    <!-- pipeline knowledge -->
    <div class="nav-title">Pipeline Knowledge</div>

      <a href="/business-templates.html">
        <div class="nav-article">Business Templates</div>
      </a>

      <a href="/template-market.html">
        <div class="nav-article">Template Market</div>
      </a>

      <!-- pipeline operation -->
      <div class="nav-title">Pipeline Operation</div>

        <a href="/template-studio.html">
          <div class="nav-article">Template Studio</div>
        </a>

        <a href="/foresight-fabric.html">
          <div class="nav-article">Foresight Fabric</div>
        </a>

    <!-- graph processing -->
    <div class="nav-title">Graph Processing</div>

      <a href="/janusgraph-ignite.html">
        <div class="nav-article">JanusGraph meets Ignite</div>
      </a>

      <a href="/graph-ingestion.html">
        <div class="nav-article">Graph Ingestion</div>
      </a>

  </div>
</nav>

            </div>
            <div class="main-content-body">
              <h1 id="named-entity-recognition">Named Entity Recognition</h1>

<ul>
  <li>Extract names of persons, organizations and locations, and time</li>
</ul>

<p>This is the initial meaning &amp; objective, but it simply means to detect a set of trained &amp; categorized “names”
in unstructured text documents. It is a matter of imagination what “name”, “document”, “sentence” or “word” means in a certain context.</p>

<p>++ Product Management Automotive</p>

<p>Surveys, Reviews und Testberichte können mit mittels Named Entity Recognition erlernte Typenbezeichnungen, wie z.B. „Z-Klasse“ oder „17er CNX“ erkennen und die vorhandenen Daten entsprechend teilen. Komplexere Named Entity Recognition Systeme, wie z.B. die von Deep Data Analytics genutzten, ermöglichen eine kleinteiligere Segmentierung der Typen Texte in unterschiedliche Attribute, z.B. Texte bei denen es um das Lenkrad oder andere Bedienelemente geht.</p>

<p>++ Customer Relationship Management</p>

<p>Oft strömen eine Vielzahl von Support Anfragen auf eine Organisation ein. Chat, E-Mail, twitter und Messangerdienste werden von Nutzern mit wachsender Begeisterung genutzt. Besonders in großen Unternehmen werden die einzelnen Kanäle von unterschiedlichen Teams betreut, so dass es schwierig ist, einen Trend in den Kundenanfragen zu identifizieren.
Named Entity Recognition kann die Gesamtheit der eingehenden Kundenkommunikation klassifizieren und wichtige Aspekte sichtbar machen. Löst ein spezifisches Produkt(Attribut) Nachfragen oder Beschwerden aus oder gibt es geographische Auffälligkeiten? NER ermöglicht es, die eingehenden Anfragen kanalübergreifend zu analysieren und nach unterschiedlichen Gesichtspunkten zu klassifizieren.</p>

<p>3.Text to Features (Feature Engineering on text data)
To analyse a preprocessed data, it needs to be converted into features. Depending upon the usage, text features can be constructed using assorted techniques – Syntactical Parsing, Entities / N-grams / word-based features, Statistical features, and word embeddings. Read on to understand these techniques in detail.</p>

<p>3.1 Syntactic Parsing
Syntactical parsing invol ves the analysis of words in the sentence for grammar and their arrangement in a manner that shows the relationships among the words. Dependency Grammar and Part of Speech tags are the important attributes of text syntactics.</p>

<p>Dependency Trees – Sentences are composed of some words sewed together. The relationship among the words in a sentence is determined by the basic dependency grammar. Dependency grammar is a class of syntactic text analysis that deals with (labeled) asymmetrical binary relations between two lexical items (words). Every relation can be represented in the form of a triplet (relation, governor, dependent). For example: consider the sentence – “Bills on ports and immigration were submitted by Senator Brownback, Republican of Kansas.” The relationship among the words can be observed in the form of a tree representation as shown:</p>

<p>The tree shows that “submitted” is the root word of this sentence, and is linked by two sub-trees (subject and object subtrees). Each subtree is a itself a dependency tree with relations such as – (“Bills” &lt;-&gt; “ports” <by> “proposition” relation), (“ports” &lt;-&gt; “immigration” <by> “conjugation” relation).</by></by></p>

<p>This type of tree, when parsed recursively in top-down manner gives grammar relation triplets as output which can be used as features for many nlp problems like entity wise sentiment analysis, actor &amp; entity identification, and text classification. The python wrapper StanfordCoreNLP (by Stanford NLP Group, only commercial license) and NLTK dependency grammars can be used to generate dependency trees.</p>

<p>Part of speech tagging – Apart from the grammar relations, every word in a sentence is also associated with a part of speech (pos) tag (nouns, verbs, adjectives, adverbs etc). The pos tags defines the usage and function of a word in the sentence. H ere is a list of all possible pos-tags defined by Pennsylvania university. Following code using NLTK performs pos tagging annotation on input text. (it provides several implementations, the default one is perceptron tagger)</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>from nltk import word_tokenize, pos_tag
text = "I am learning Natural Language Processing on Analytics Vidhya"
tokens = word_tokenize(text)
print pos_tag(tokens)
&gt;&gt;&gt; [('I', 'PRP'), ('am', 'VBP'), ('learning', 'VBG'), ('Natural', 'NNP'),('Language', 'NNP'),
('Processing', 'NNP'), ('on', 'IN'), ('Analytics', 'NNP'),('Vidhya', 'NNP')]
</code></pre></div></div>
<p>Part of Speech tagging is used for many important purposes in NLP:</p>

<p>A.Word sense disambiguation: Some language words have multiple meanings according to their usage. For example, in the two sentences below:</p>

<p>I. “Please book my flight for Delhi”</p>

<p>II. “I am going to read this book in the flight”</p>

<p>“Book” is used with different context, however the part of speech tag for both of the cases are different. In sentence I, the word “book” is used as v erb, while in II it is used as no un. (Lesk Algorithm is also us ed for similar purposes)</p>

<p>B.Improving word-based features: A learning model could learn different contexts of a word when used word as the features, however if the part of speech tag is linked with them, the context is preserved, thus making strong features. For example:</p>

<p>Sentence -“book my flight, I will read this book”</p>

<p>Tokens – (“book”, 2), (“my”, 1), (“flight”, 1), (“I”, 1), (“will”, 1), (“read”, 1), (“this”, 1)</p>

<p>Tokens with POS – (“book_VB”, 1), (“my_PRP$”, 1), (“flight_NN”, 1), (“I_PRP”, 1), (“will_MD”, 1), (“read_VB”, 1), (“this_DT”, 1), (“book_NN”, 1)</p>

<p>C. Normalization and Lemmatization: POS tags are the basis of lemmatization process for converting a word to its base form (lemma).</p>

<p>D.Efficient stopword removal : P OS tags are also useful in efficient removal of stopwords.</p>

<p>For example, there are some tags which always define the low frequency / less important words of a language. For example: (IN – “within”, “upon”, “except”), (CD – “one”,”two”, “hundred”), (MD – “may”, “mu st” etc)</p>

<p>3.2 Entity Extraction (Entities as features)
Entities are defined as the most important chunks of a sentence – noun phrases, verb phrases or both. Entity Detection algorithms are generally ensemble models of rule based parsing, dictionary lookups, pos tagging and dependency parsing. The applicability of entity detection can be seen in the automated chat bots, content analyzers and consumer insights.</p>

<p>Topic Modelling &amp; Named Entity Recognition are the two key entity detection methods in NLP.</p>

<p>A. Named Entity Recognition (NER)
The process of detecting the named entities such as person names, location names, company names etc from the text is called as NER. For example :</p>

<p>Sentence – Sergey Brin, the manager of Google Inc. is walking in the streets of New York.</p>

<p>Named Entities –  ( “person” : “Sergey Brin” ), (“org” : “Google Inc.”), (“location” : “New York”)</p>

<p>A typical NER model consists of three blocks:</p>

<p>Noun phrase identification: This step deals with extracting all the noun phrases from a text using dependency parsing and part of speech tagging.</p>

<p>Phrase classification: This is the classification step in which all the extracted noun phrases are classified into respective categories (locations, names etc). Google Maps API provides a good path to disambiguate locations, Then, the open databases from dbpedia, wikipedia can be used to identify person names or company names. Apart from this, one can curate the lookup tables and dictionaries by combining information from different sources.</p>

<p>Entity disambiguation: Sometimes it is possible that entities are misclassified, hence creating a validation layer on top of the results is useful. Use of knowledge graphs can be exploited for this purposes. The popular knowledge graphs are – Google Knowledge Graph, IBM Watson and Wikipedia.</p>

<p>B. Topic Modeling
Topic modeling is a process of automatically identifying the topics present in a text corpus, it derives the hidden patterns among the words in the corpus in an unsupervised manner. Topics are defined as “a repeating pattern of co-occurring terms in a corpus”. A good topic model results in – “health”, “doctor”, “patient”, “hospital” for a topic – Healthcare, and “farm”, “crops”, “wheat” for a topic – “Farming”.</p>

<p>Latent Dirichlet Allocation (LDA) is the most popular topic modelling technique, Following is the code to implement topic modeling using LDA in python. For a detailed explanation about its working and implementation, check the complete article here.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>doc1 = "Sugar is bad to consume. My sister likes to have sugar, but not my father."
doc2 = "My father spends a lot of time driving my sister around to dance practice."
doc3 = "Doctors suggest that driving may cause increased stress and blood pressure."
doc_complete = [doc1, doc2, doc3]
doc_clean = [doc.split() for doc in doc_complete]

import gensim from gensim
import corpora

# Creating the term dictionary of our corpus, where every unique term is assigned an index.  
dictionary = corpora.Dictionary(doc_clean)

# Converting list of documents (corpus) into Document Term Matrix using dictionary prepared above.
doc_term_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]

# Creating the object for LDA model using gensim library
Lda = gensim.models.ldamodel.LdaModel

# Running and Training LDA model on the document term matrix
ldamodel = Lda(doc_term_matrix, num_topics=3, id2word = dictionary, passes=50)

# Results
print(ldamodel.print_topics())

</code></pre></div></div>

<p>C.  N-Grams as Features
A combination of N words together are called N-Grams. N grams (N &gt; 1) are generally more informative as compared to words (Unigrams) as features. Also, bigrams (N = 2) are considered as the most important features of all the others. The following code generates bigram of a text.</p>

<div class="highlighter-rouge"><div class="highlight"><pre class="highlight"><code>def generate_ngrams(text, n):
    words = text.split()
    output = []  
    for i in range(len(words)-n+1):
        output.append(words[i:i+n])
    return output

&gt;&gt;&gt; generate_ngrams('this is a sample text', 2)
# [['this', 'is'], ['is', 'a'], ['a', 'sample'], , ['sample', 'text']]
</code></pre></div></div>

            </div>
          </div>
        </div>
        <div class="column-right">

          <div id="content-toc">

            <div class="toc-header">
              <i class="fa fa-bars"></i><span>Table of Content</span>
            </div>
            <div class="toc-body">
              
            </div>
          </div>

        </div>
      </div>

      
<div class="site-footer">

    <div class="column-left"></div>
    <div class="column-content">
      Predictive<span class="brand-teal">Works.</span> is an initiative of Dr. Krusche & Partner PartG - (c) 2020. All rights reserved.
    </div>
    <div class="column-right"></div>

</div>


  </body>
</html>
